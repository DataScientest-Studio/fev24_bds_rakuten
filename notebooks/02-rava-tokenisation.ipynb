{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Constantes\n",
    "PATH_DATA = \"../data/\"\n",
    "PATH_RAW = PATH_DATA + \"raw/\"\n",
    "PATH_PROCESSED = PATH_DATA + \"processed/\"\n",
    "PATH_EXTERNAL = PATH_DATA + \"external/\"\n",
    "SEED = 123\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmes</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_lemmes</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>pages de carnet personnalisées olivia dot grid...</td>\n",
       "      <td>page de carnet personnalisé olivia dot grid ca...</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>journal des arts le n du l art et son marche s...</td>\n",
       "      <td>journal des art le ne du l art et son marche s...</td>\n",
       "      <td>177</td>\n",
       "      <td>172</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>731</td>\n",
       "      <td>732</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>peluche donald europe disneyland marionnette à...</td>\n",
       "      <td>peluche donald europe disneyland marionnette à...</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>la guerre des tuques luc a des idées de grande...</td>\n",
       "      <td>la guerre des tuques luc a des idée de grandeu...</td>\n",
       "      <td>203</td>\n",
       "      <td>212</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid                                               text  \\\n",
       "0  3804725264  1263597046  pages de carnet personnalisées olivia dot grid...   \n",
       "1   436067568  1008141237  journal des arts le n du l art et son marche s...   \n",
       "2   201115110   938777978  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3    50418756   457047496  peluche donald europe disneyland marionnette à...   \n",
       "4   278535884  1077757786  la guerre des tuques luc a des idées de grande...   \n",
       "\n",
       "                                              lemmes  len_text  len_lemmes  \\\n",
       "0  page de carnet personnalisé olivia dot grid ca...        68          64   \n",
       "1  journal des art le ne du l art et son marche s...       177         172   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...       731         732   \n",
       "3  peluche donald europe disneyland marionnette à...        52          52   \n",
       "4  la guerre des tuques luc a des idée de grandeu...       203         212   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  \n",
       "2           50  \n",
       "3         1280  \n",
       "4         2705  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 1)\n",
      "498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'à', 'â', 'abord', 'afin', 'ah', 'ai', 'aie', 'ainsi', 'allaient']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PATH_PROCESSED + \"X_train_preprocessed.csv\", index_col=0)\n",
    "display(df.head())\n",
    "df.describe()\n",
    "\n",
    "stop_words_french = pd.read_json(PATH_EXTERNAL + \"stop_words_french.json\")\n",
    "print(stop_words_french.shape)\n",
    "stop_words = []\n",
    "stop_words.extend(stop_words_french[0].tolist())\n",
    "stop_words.extend([\"cm\", \"mm\"])\n",
    "print(len(stop_words))\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"lemmes\"]\n",
    "target = df[\"prdtypecode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67932,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16984,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=SEED\n",
    ")\n",
    "print(X_train.shape)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "Voici les paramètres qui me semblent importants pour une classification de texte sur un corpus de 90 000 textes d'environ 500 caractères chacun:\n",
    "\n",
    "- Le paramètre <b>ngram_range</b>, qui permet de générer des n-grams de caractères ou de mots. Pour un texte court, les unigrams et bigrams seraient probablement suffisants. \n",
    "\n",
    "- Le paramètre <b>max_features</b>, qui limite le nombre de features conservées. Étant donné le grand nombre de textes, il serait sage de limiter le nombre de features pour éviter le surapprentissage. \n",
    "\n",
    "- Le paramètre <b>min_df</b>, qui ignore les mots apparaissant dans trop peu de documents. Ici, on peut ignorer les mots n'apparaissant que dans quelques dizaines de textes sur 90 000.\n",
    "\n",
    "- Le paramètre <b>max_df</b>, qui ignore les mots trop fréquents comme les mots vides. Avec des textes courts, de nombreux mots apparaitront dans une grande proportion des textes.\n",
    "\n",
    "- Le paramètre <b>stop_words</b>, pour retirer les mots vides de la langue du corpus.\n",
    "\n",
    "- Le paramètre <b>lowercase</b>, pour mettre tous les mots en minuscules et ainsi neutraliser les variations casse.\n",
    "\n",
    "Voilà les principaux paramètres à prendre en compte pour cette tâche de classification de texte sur un grand corpus de textes courts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremyrava/anaconda3/envs/datascience_py3_11_7/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.5,\n",
    "    min_df=0.0001,\n",
    "    lowercase=True,\n",
    "    stop_words=stop_words,\n",
    "    max_features=10000,\n",
    ")\n",
    "\n",
    "X_train_vec = tfidf_vect.fit_transform(X_train)\n",
    "X_test_vec = tfidf_vect.transform(X_test)\n",
    "\n",
    "# nombre de features\n",
    "len(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors\n",
    "Attention au nombre de features, prend beaucoup de temps si features important\n",
    "\n",
    "le résultat ici est sur `max_features=2000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "0.3934831453134052\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "parameter_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\"],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object with defined parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=KNeighborsRegressor(), param_grid=parameter_grid, cv=5\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremyrava/anaconda3/envs/datascience_py3_11_7/lib/python3.11/site-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train : 0.5698129727061936\n",
      "Score test : 0.3650525454508564\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3, weights=\"distance\", algorithm=\"auto\")\n",
    "\n",
    "knn.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Score train :\", knn.score(X_train_vec, y_train))\n",
    "print(\"Score test :\", knn.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Ensemble Classifiers\n",
    "Attention au nombre de features, prend beaucoup de temps si features important\n",
    "\n",
    "le résultat ici est sur `max_features=2000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train : 0.7938232349997056\n",
      "Score test : 0.7005416862929816\n"
     ]
    }
   ],
   "source": [
    "clf_svc = SVC()\n",
    "\n",
    "clf_svc.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Score train :\", clf_svc.score(X_train_vec, y_train))\n",
    "print(\"Score test :\", clf_svc.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "D'après la page, les modèles de classification naïve bayésienne sont souvent bien adaptés aux données de classification de texte, car ils nécessitent peu de données d'entraînement et sont très rapides par rapport à d'autres méthodes plus sophistiquées. \n",
    "\n",
    "En particulier, le modèle \"Multinomial Naive Bayes\" est l'un des deux modèles naïfs bayésiens classiques utilisés pour la classification de texte, car il suppose une distribution multinomiale des données, ce qui convient bien aux vecteurs de comptage de mots qui représentent généralement les données textuelles. \n",
    "\n",
    "Le modèle \"Complement Naive Bayes\" est également mentionné comme s'adaptant particulièrement bien aux ensembles de données déséquilibrés, en utilisant des statistiques du \"complément\" de chaque classe pour calculer les poids du modèle.\n",
    "\n",
    "Donc en résumé, les modèles de classification naïve bayésienne comme \"Multinomial Naive Bayes\" et \"Complement Naive Bayes\" sont souvent les mieux adaptés pour la classification de texte avec des entrées textuelles et une cible de classification.\n",
    "\n",
    "#### Peu importe le nombre de `max_features`, le résultat est instantané"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train : 0.7669434140022375\n",
      "Score test : 0.7393429109750354\n"
     ]
    }
   ],
   "source": [
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Score train :\", clf_mnb.score(X_train_vec, y_train))\n",
    "print(\"Score test :\", clf_mnb.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train : 0.7384590472825767\n",
      "Score test : 0.716674517192652\n"
     ]
    }
   ],
   "source": [
    "clf_cnb = ComplementNB()\n",
    "clf_cnb.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Score train :\", clf_cnb.score(X_train_vec, y_train))\n",
    "print(\"Score test :\", clf_cnb.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation croisée imbriquée (Nested CV)\n",
    "Pas encore testé (ne fonctionne pas actuellement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=22, max_iter=2000)\n",
    "rf = RandomForestClassifier(random_state=22)\n",
    "svc = SVC(random_state=22)\n",
    "\n",
    "param_grid_lr = {\"solver\": [\"liblinear\", \"lbfgs\"], \"C\": np.logspace(-4, 2, 9)}\n",
    "\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        \"n_estimators\": [10, 50, 100, 250, 500, 1000],\n",
    "        \"min_samples_leaf\": [1, 3, 5],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grid_svc = [\n",
    "    {\"kernel\": [\"rbf\"], \"C\": np.logspace(-4, 4, 9), \"gamma\": np.logspace(-4, 0, 4)},\n",
    "    {\"kernel\": [\"linear\"], \"C\": np.logspace(-4, 4, 9)},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridcvs = {\n",
    "    \"lr\": GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=3, refit=True),\n",
    "    \"rf\": GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=3, refit=True),\n",
    "    \"svc\": GridSearchCV(estimator=svc, param_grid=param_grid_svc, cv=3, refit=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "outer_scores = {}\n",
    "for k, v in gridcvs.items():\n",
    "    outer_scores[k] = cross_val_score(v, X_train, y_train, cv=skf)\n",
    "    print(\n",
    "        f\"{k}: outer accuracy {100*outer_scores[k].mean():.2f} +/- {100*outer_scores[k].std():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = gridcvs[\"rf\"]\n",
    "final_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {final_clf.best_params_}\")\n",
    "\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=final_clf.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=final_clf.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {100*train_acc:.2f}\")\n",
    "print(f\"Test Accuracy: {100*test_acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
