{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, string\n",
    "from skimage import io, color, feature, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"../../mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des chemins\n",
    "images_path = \"/Users/jeremyrava/Documents/01 - Projets/fev24_bds_rakuten/data/raw/images/image_train\"\n",
    "X_csv_path = (\n",
    "    \"/Users/jeremyrava/Documents/01 - Projets/fev24_bds_rakuten/data/raw/x_train.csv\"\n",
    ")\n",
    "y_csv_path = (\n",
    "    \"/Users/jeremyrava/Documents/01 - Projets/fev24_bds_rakuten/data/raw/y_train.csv\"\n",
    ")\n",
    "\n",
    "# Chargement des données\n",
    "X_df = pd.read_csv(X_csv_path, index_col=0)\n",
    "y_df = pd.read_csv(y_csv_path, index_col=0)\n",
    "\n",
    "X_df[\"text\"] = np.where(\n",
    "    X_df[\"description\"].isna(),\n",
    "    X_df[\"designation\"].astype(str),\n",
    "    X_df[\"designation\"].astype(str) + \" \" + X_df[\"description\"].astype(str),\n",
    ")\n",
    "\n",
    "# Réduire les données aux 5000 premières lignes\n",
    "sample_X = X_df\n",
    "target = y_df[\"prdtypecode\"]\n",
    "\n",
    "# Ajout du chemin complet des images dans sample_X\n",
    "sample_X[\"image_path\"] = sample_X.apply(\n",
    "    lambda row: os.path.join(\n",
    "        images_path, f\"image_{row.imageid}_product_{row.productid}.jpg\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data = sample_X[[\"text\", \"image_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 11:31:58.008092: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-05-09 11:31:58.008111: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-09 11:31:58.008129: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-09 11:31:58.008157: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-09 11:31:58.008172: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \"\"\"\n",
    "    Custom standardization function for text data.\n",
    "\n",
    "    Args:\n",
    "        input_data: The input text data.\n",
    "\n",
    "    Returns:\n",
    "        The standardized text data.\n",
    "    \"\"\"\n",
    "    # Decode the input HTML using UTF-8 encoding.\n",
    "    decoded_html = tf.strings.unicode_decode(input_data, \"UTF-8\")\n",
    "\n",
    "    # Encode the decoded HTML back into HTML for further processing.\n",
    "    encoded_html = tf.strings.unicode_encode(decoded_html, \"UTF-8\")\n",
    "\n",
    "    # Strip all HTML tags from the input data using a regular expression replace operation.\n",
    "    stripped_html = tf.strings.regex_replace(encoded_html, \"<[^>]*>\", \" \")\n",
    "\n",
    "    # Convert the input text to lowercase for consistency.\n",
    "    lowercase = tf.strings.lower(stripped_html)\n",
    "\n",
    "    # Remove extra whitespace by replacing one or more spaces with a single space.\n",
    "    cleaned_input_data = tf.strings.regex_replace(lowercase, r\"\\s+\", \" \")\n",
    "\n",
    "    # Replace punctuation characters with empty strings (i.e., remove them).\n",
    "    return tf.strings.regex_replace(\n",
    "        cleaned_input_data, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=100000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=250,\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "vectorize_layer.adapt(sample_X[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 10:01:09.659414: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-05-09 10:01:09.659445: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-09 10:01:09.659453: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-09 10:01:09.659714: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-09 10:01:09.659735: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Configuration pour la vectorisation du texte\n",
    "max_tokens = 10000\n",
    "output_sequence_length = 250\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=output_sequence_length,\n",
    ")\n",
    "vectorize_layer.adapt(sample_X[\"description\"].fillna(\"\"))\n",
    "\n",
    "# Appliquer la vectorisation au texte pour obtenir les vecteurs\n",
    "X_text_vectors = vectorize_layer(sample_X[\"description\"].fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les caractéristiques HOG d'une image\n",
    "def extract_hog_features(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    image_gray = color.rgb2gray(image)\n",
    "    image_resized = transform.resize(image_gray, (128, 64), anti_aliasing=True)\n",
    "    hog_features = feature.hog(\n",
    "        image_resized, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=False\n",
    "    )\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_text vectorised\n",
      "X_test_text vectorised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a078dde75a459eaee5bca9522d4650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8643be340b46b7952e674e66cffd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Appliquer la vectorisation au texte pour obtenir les vecteurs\n",
    "X_train_text_vectors = vectorize_layer(X_train[\"text\"])\n",
    "print(\"X_train_text vectorised\")\n",
    "X_test_text_vectors = vectorize_layer(X_test[\"text\"])\n",
    "print(\"X_test_text vectorised\")\n",
    "\n",
    "# Préparation des caractéristiques HOG pour les images\n",
    "X_train_features_images = np.array(\n",
    "    [extract_hog_features(path) for path in tqdm(X_train[\"image_path\"])]\n",
    ")\n",
    "X_test_features_images = np.array(\n",
    "    [extract_hog_features(path) for path in tqdm(X_test[\"image_path\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(log_datasets=False, disable=True)  # pour ne pas log\n",
    "# Rééquilibrage des classes avec RandomOverSampler pour les deux types de données\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled_text, y_resampled_text = ros.fit_resample(\n",
    "    X_train_text_vectors, y_train\n",
    ")\n",
    "X_train_resampled_images, y_resampled_images = ros.fit_resample(\n",
    "    X_train_features_images, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des caractéristiques image et texte (sans conversion à dense si déjà en numpy array)\n",
    "X_train_combined = np.hstack((X_train_resampled_images, X_train_resampled_text))\n",
    "X_test_combined = np.hstack((X_test_features_images, X_test_text_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run id: 471d777b219546fc95a77aa6c782dca8\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=10;, score=0.709 total time= 1.2min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=10;, score=0.700 total time= 1.2min\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=10;, score=0.739 total time= 1.3min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=10;, score=0.743 total time= 1.3min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=10;, score=0.730 total time= 1.3min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=10;, score=0.695 total time= 1.0min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=10;, score=0.698 total time= 1.0min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=10;, score=0.717 total time=  53.8s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=10;, score=0.733 total time=  54.3s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=10;, score=0.729 total time=  51.3s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.846 total time= 4.8min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.805 total time= 4.9min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.850 total time= 4.9min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.814 total time= 4.9min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.834 total time= 4.9min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.803 total time= 4.2min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.793 total time= 4.2min\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.833 total time= 4.0min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.818 total time= 4.1min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.824 total time= 9.0min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.836 total time= 4.2min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.833 total time= 9.2min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=10;, score=0.674 total time=  52.7s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.869 total time= 8.6min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=10;, score=0.669 total time=  53.5s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=10;, score=0.693 total time=  52.4s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.865 total time= 8.7min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.851 total time= 8.7min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=10;, score=0.706 total time=  54.7s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=10;, score=0.706 total time=  51.7s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.813 total time= 8.2min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.823 total time= 8.3min\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.804 total time= 3.8min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.769 total time= 3.9min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.808 total time= 3.9min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.792 total time= 3.9min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.780 total time= 4.0min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.838 total time= 7.6min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.855 total time= 7.3min\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.852 total time= 7.1min\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.800 total time= 6.6min\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.790 total time= 6.3min\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.812 total time= 5.5min\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.828 total time= 5.5min\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.824 total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2916: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "2024/05/09 12:12:14 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/anaconda3/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "2024/05/09 12:12:19 INFO mlflow.sklearn.utils: Logging the 5 best runs, 4 runs will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.8484076293421952\n"
     ]
    }
   ],
   "source": [
    "# Utiliser un modèle unique pour la classification sur les caractéristiques combinées\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "combined_model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    combined_model, param_grid, cv=5, scoring=\"f1_weighted\", verbose=3, n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"features RF\")\n",
    "mlflow.sklearn.autolog(log_datasets=False, disable=False)\n",
    "description = \"SMOTE(random_state=42) sur la totlité de X_train (sans diminuer les ecarts des targets)\"\n",
    "with mlflow.start_run(description=description) as run:\n",
    "    print(\"Run id:\", run.info.run_id)\n",
    "    grid_search.fit(\n",
    "        X_train_combined, y_resampled_text\n",
    "    )  # y_train_images doit être identique à y_train_text\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(best_params)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Model Accuracy: 0.4822774375883184\n",
      "Classification Report for Combined Features Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.37      0.59      0.45       612\n",
      "          40       0.47      0.45      0.46       521\n",
      "          50       0.35      0.24      0.28       357\n",
      "          60       0.22      0.61      0.32       161\n",
      "        1140       0.33      0.47      0.39       539\n",
      "        1160       0.79      0.76      0.78       786\n",
      "        1180       0.33      0.23      0.27       146\n",
      "        1280       0.24      0.12      0.16       961\n",
      "        1281       0.22      0.08      0.11       424\n",
      "        1300       0.40      0.40      0.40       974\n",
      "        1301       0.57      0.53      0.55       169\n",
      "        1302       0.34      0.28      0.31       507\n",
      "        1320       0.42      0.24      0.31       672\n",
      "        1560       0.37      0.42      0.39      1013\n",
      "        1920       0.69      0.70      0.69       841\n",
      "        1940       0.26      0.47      0.33       137\n",
      "        2060       0.38      0.43      0.40      1029\n",
      "        2220       0.16      0.14      0.15       170\n",
      "        2280       0.53      0.63      0.57       942\n",
      "        2403       0.42      0.53      0.47       986\n",
      "        2462       0.30      0.32      0.31       306\n",
      "        2522       0.66      0.56      0.60       991\n",
      "        2582       0.31      0.29      0.30       462\n",
      "        2583       0.76      0.70      0.73      2047\n",
      "        2585       0.44      0.30      0.36       525\n",
      "        2705       0.62      0.75      0.68       517\n",
      "        2905       0.80      0.96      0.87       189\n",
      "\n",
      "    accuracy                           0.48     16984\n",
      "   macro avg       0.43      0.45      0.43     16984\n",
      "weighted avg       0.48      0.48      0.48     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modele_selected = \"471d777b219546fc95a77aa6c782dca8\"  # ID du run du modèle sélectionné\n",
    "logged_model = f\"runs:/{modele_selected}/best_estimator\"  # choisir \"/model\" pour model standard ou \"/best_estimator\" pour un GridSearchCV\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.sklearn.load_model(logged_model)\n",
    "\n",
    "# Prédiction et évaluation\n",
    "y_pred_combined = loaded_model.predict(X_test_combined)\n",
    "print(\"Combined Features Model Accuracy:\", accuracy_score(y_test, y_pred_combined))\n",
    "print(\"Classification Report for Combined Features Model:\")\n",
    "print(classification_report(y_test, y_pred_combined))\n",
    "report = classification_report(y_test, y_pred_combined, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv(f\"report_Features_RF_{modele_selected}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
