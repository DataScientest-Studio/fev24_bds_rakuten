{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import (\n",
    "    TextVectorization,\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from keras import Sequential, losses, optimizers, metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "# Constantes\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \"\"\"\n",
    "    Custom standardization function for text data.\n",
    "\n",
    "    Args:\n",
    "        input_data: The input text data.\n",
    "\n",
    "    Returns:\n",
    "        The standardized text data.\n",
    "    \"\"\"\n",
    "    decoded_html = tf.strings.unicode_decode(input_data, \"UTF-8\")\n",
    "    encoded_html = tf.strings.unicode_encode(decoded_html, \"UTF-8\")\n",
    "    stripped_html = tf.strings.regex_replace(encoded_html, \"<[^>]*>\", \" \")\n",
    "    lowercase = tf.strings.lower(stripped_html)\n",
    "    cleaned_input_data = tf.strings.regex_replace(lowercase, r\"\\s+\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        cleaned_input_data, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def plt_graph(training_history, run=None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "    range_epochs = np.arange(1, len(training_history.epoch) + 1, 1)\n",
    "\n",
    "    # Courbe de la précision sur l'échantillon d'entrainement\n",
    "    ax1.plot(\n",
    "        range_epochs,\n",
    "        training_history.history[\"accuracy\"],\n",
    "        label=\"Training Accuracy\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    ax1.plot(\n",
    "        range_epochs,\n",
    "        training_history.history[\"val_accuracy\"],\n",
    "        label=\"Validation Accuracy\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(\"Training and Validation Accuracy\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Courbe de la précision sur l'échantillon de test\n",
    "    ax2.plot(\n",
    "        range_epochs,\n",
    "        training_history.history[\"loss\"],\n",
    "        label=\"Training Loss\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax2.plot(\n",
    "        range_epochs,\n",
    "        training_history.history[\"val_loss\"],\n",
    "        label=\"Validation Loss\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(\"Training and Validation Loss\")\n",
    "    ax2.legend()\n",
    "\n",
    "    if run != None:\n",
    "        id_exp = str(run.info.run_id)\n",
    "    else:\n",
    "        id_exp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.savefig(f\"reports/figures/{id_exp}_training_history_DNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"mlruns\")  # \"http://127.0.0.1:8080\"\n",
    "mlflow.set_experiment(experiment_name=\"voting_classifier_model_DNN_CNN_NetBO\")\n",
    "mlflow.tensorflow.autolog(log_datasets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/raw/x_train.csv\", index_col=0)\n",
    "df_target = pd.read_csv(\"data/raw/y_train.csv\", index_col=0)\n",
    "df[df_target.columns[0]] = df_target\n",
    "\n",
    "df[\"text\"] = np.where(\n",
    "    df[\"description\"].isna(),\n",
    "    df[\"designation\"].astype(str),\n",
    "    df[\"designation\"].astype(str) + \" \" + df[\"description\"].astype(str),\n",
    ")\n",
    "\n",
    "df.drop(\"designation\", axis=1, inplace=True)\n",
    "df.drop(\"description\", axis=1, inplace=True)\n",
    "df.drop(\"productid\", axis=1, inplace=True)\n",
    "df.drop(\"imageid\", axis=1, inplace=True)\n",
    "\n",
    "num_classes = df[\"prdtypecode\"].value_counts().shape[0]\n",
    "\n",
    "data = df[\"text\"]\n",
    "target = df[\"prdtypecode\"].astype(\"str\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Encode\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Vectorize\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"\\ny_train shape:\", y_train.shape)\n",
    "\n",
    "y_train_1d = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Appliquer des poids aux classes selon l'équilibrage du dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_1d), y=y_train_1d\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping class indices to their corresponding weights\n",
    "class_weight_dict = dict(zip(np.unique(y_train_1d), class_weights))\n",
    "\n",
    "# Défnit la longueur de la séquence du model\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "# median = 320\n",
    "# mean = 600\n",
    "df[\"len\"] = df[\"text\"].str.len()\n",
    "sequence_length = 5000\n",
    "print(\"\\nsequence_length:\", sequence_length, \"\\n\")\n",
    "\n",
    "# Pour libérer de la RAM\n",
    "del df, data, df_target, target\n",
    "\n",
    "X_train = tf.strings.as_string(X_train)\n",
    "X_test = tf.strings.as_string(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=100000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "vectorize_layer.adapt(X_train)\n",
    "print(\"\\nX_train shape: \", X_train.shape, \"\\n\")\n",
    "\n",
    "print(tf.config.list_physical_devices(\"GPU\"), \"\\n\")\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        vectorize_layer,\n",
    "        Embedding(len(vectorize_layer.get_vocabulary()), 200, mask_zero=True),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "opt = optimizers.Adam(0.001)\n",
    "loss = losses.CategoricalCrossentropy()\n",
    "f1_score = metrics.F1Score(average=\"weighted\")\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,\n",
    "    metrics=[\"accuracy\", f1_score],\n",
    ")\n",
    "\n",
    "model.build((None, sequence_length))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=4,  # Attendre n epochs avant application\n",
    "    min_delta=0.0001,  # si au bout de n epochs la fonction de perte ne varie pas de n %,\n",
    "    # que ce soit à la hausse ou à la baisse, on arrête\n",
    "    verbose=1,  # Afficher à quel epoch on s'arrête\n",
    "    monitor=\"val_loss\",\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,  # si val_loss stagne sur n epochs consécutives selon la valeur min_delta\n",
    "    min_delta=0.005,\n",
    "    min_lr=0.00001,\n",
    "    factor=0.1,  # On réduit le learning rate d'un facteur n\n",
    "    cooldown=2,  # On attend n epochs avant de réitérer\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    print(\"Run id:\", run.info.run_id)\n",
    "\n",
    "    # Train the model\n",
    "    training_history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=20,\n",
    "        batch_size=128 * 4,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[reduce_learning_rate, early_stopping],\n",
    "        class_weight=class_weight_dict,\n",
    "    )\n",
    "    plt_graph(training_history, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
