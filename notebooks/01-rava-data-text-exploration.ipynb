{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm.auto import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import html\n",
    "from langdetect import detect, detect_langs, LangDetectException\n",
    "from google_trans_new import google_translator\n",
    "import random\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap to Text Mining\n",
    "\n",
    "Text mining est le processus d'extraction d'informations à partir de grandes quantités de données non structurées. Cela implique plusieurs étapes, qui peuvent être décomposées en ces étapes suivantes :\n",
    "\n",
    "1. Collecte des données : La première étape du text mining est la collecte de grande quantité de données non structurées. Cette donnée peut provenir de diverses sources telles que les plateformes sociaux, les sites Web, les blogs et les articles de presse.\n",
    "\n",
    "2. Prétraitement : Une fois les données recueillies, elles doivent être prétraitées. Cela consiste à nettoyer les données en supprimant les caractères indésirables ou les mots, en corrigeant les erreurs orthographiques et en convertissant le texte en minuscule.\n",
    "\n",
    "3. Tokenisation : Une fois les données prétraitées, elles sont tokenisées. Cela signifie que le texte est divisé en mots individuels ou des tokens. Cette étape est importante car elle permet d'analyser la fréquence de chaque mot dans le texte.\n",
    "\n",
    "4. Suppression des mots-clés : Une fois le texte tokenisé, nous pouvons supprimer les mots-clés. Les mots-clés sont des mots courants tels que \"le\", \"et\" et \"est\" qui n'apportent pas de sens particulier et peuvent être supprimés sans affecter l'analyse globale.\n",
    "\n",
    "5. Reduction morphologique ou normalisation du lemme : La prochaine étape est la reduction morphologique ou la normalisation du lemme. Cela consiste à réduire chaque mot à sa forme de base, également connu sous le nom de racine de mot. Par exemple, \"courir\" serait réduit à \"court\". Cela est fait pour regrouper des mots similaires ensemble, comme différents temps verbaux du même mot.\n",
    "\n",
    "6. Extraction des fonctionnalités : Une fois le texte prétraité et nettoyé, nous pouvons extraire les fonctionnalités à partir de lui. Ces fonctionnalités peuvent inclure des choses telles que la fréquence des mots, l'analyse du sentiment ou le modélisation des sujets.\n",
    "\n",
    "7. Analyse : Enfin, nous pouvons analyser les données à l'aide de diverses techniques telles que l'agrégation ou la classification pour obtenir des informations sur le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "FIGSIZE = (12, 4)\n",
    "PATH_DATA = \"../data/\"\n",
    "PATH_RAW = PATH_DATA + \"raw/\"\n",
    "PATH_PROCESSED = PATH_DATA + \"processed/\"\n",
    "PATH_EXTERNAL = PATH_DATA + \"external/\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_RAW + \"x_train.csv\", index_col=0)\n",
    "target = pd.read_csv(PATH_RAW + \"y_train.csv\", index_col=0)\n",
    "display(df.head())\n",
    "display(target.describe())\n",
    "df_origin = pd.read_csv(PATH_RAW + \"x_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_french = pd.read_json(PATH_EXTERNAL + \"stop_words_french.json\")\n",
    "print(stop_words_french.shape)\n",
    "stop_words = []\n",
    "stop_words.extend(stop_words_french[0].tolist())\n",
    "stop_words.extend([\"cm\", \"mm\"])\n",
    "print(len(stop_words))\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING LEXIQUE FRANÇAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(texts):\n",
    "    sentences = []\n",
    "    for sentence in tqdm(texts):\n",
    "        if sentence is np.nan:\n",
    "            sentences.append(np.nan)\n",
    "        else:\n",
    "            s = \"\".join(\n",
    "                c\n",
    "                for c in unicodedata.normalize(\"NFD\", sentence)\n",
    "                if unicodedata.category(c) != \"Mn\"\n",
    "            )\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexique = pd.read_table(PATH_EXTERNAL + \"Lexique383/Lexique383.tsv\", delimiter=\"\\t\")\n",
    "lexique_filtred_unique = lexique.drop_duplicates(subset=\"ortho\")[[\"ortho\", \"lemme\"]]\n",
    "lexique_filtred_unique.dropna(subset=[\"ortho\", \"lemme\"], axis=\"index\", inplace=True)\n",
    "lexique_filtred_unique[\"ortho\"] = strip_accents(lexique_filtred_unique[\"ortho\"])\n",
    "lexique_filtred_unique[\"lemme\"] = strip_accents(lexique_filtred_unique[\"lemme\"])\n",
    "lexique_filtred_unique = lexique_filtred_unique.drop_duplicates(subset=\"ortho\")[\n",
    "    [\"ortho\", \"lemme\"]\n",
    "]\n",
    "\n",
    "lexique_filtred_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premier nettoyage\n",
    "### pour pouvoir traduire les textes d'une autre langue en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_cleaning(sentences):\n",
    "    # Pre-compile regex patterns\n",
    "    HTML_TAGS_RE = re.compile(\"<[^>]*\")\n",
    "    URL_RE = re.compile(r\"https?://[-_.?&~;+=/#0-9A-Za-z]{1,2076}\")\n",
    "    MAIL_RE = re.compile(\n",
    "        r\"[-_.0-9A-Za-z]{1,64}@[-_0-9A-Za-z]{1,255}[-_.0-9A-Za-z]{1,255}\"\n",
    "    )\n",
    "    SPE_CHAR_RE = re.compile(\"[^a-zA-ZÀ-ÿ]\")\n",
    "    SPACES_RE = re.compile(\"\\s+\")\n",
    "    SPACES_RE_2 = re.compile(\" +\")\n",
    "\n",
    "    sentences_cleaned = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Decode HTML entities\n",
    "        sentence = html.unescape(sentence)\n",
    "\n",
    "        # Replace HTML tags with spaces\n",
    "        sentence = HTML_TAGS_RE.sub(\"\", sentence)\n",
    "\n",
    "        # Remove URL and e-mail adress\n",
    "        sentence = URL_RE.sub(\"\", sentence)\n",
    "        sentence = MAIL_RE.sub(\"\", sentence)\n",
    "\n",
    "        # Replace special characters with spaces while keeping accents\n",
    "        sentence = SPE_CHAR_RE.sub(\" \", sentence)\n",
    "\n",
    "        # Remove unnecessary spaces from the sentence using regular expressions\n",
    "        cleaned_sentence = SPACES_RE.sub(\" \", sentence)\n",
    "        cleaned_sentence = SPACES_RE_2.sub(\" \", cleaned_sentence)\n",
    "        sentences_cleaned.append(cleaned_sentence)\n",
    "\n",
    "    return sentences_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"designation\"] = first_cleaning(df[\"designation\"])\n",
    "df[\"description\"].loc[~df[\"description\"].isna()] = first_cleaning(\n",
    "    df[\"description\"].loc[~df[\"description\"].isna()]\n",
    ")\n",
    "\n",
    "# join texts\n",
    "df[\"text\"] = np.where(\n",
    "    df[\"description\"].isna(),\n",
    "    df[\"designation\"].astype(str),\n",
    "    df[\"designation\"].astype(str) + \" \" + df[\"description\"].astype(str),\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection des langues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
