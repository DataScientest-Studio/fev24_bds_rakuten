{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeremyrava/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm.auto import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import html\n",
    "from langdetect import detect, detect_langs, LangDetectException\n",
    "from google_trans_new import google_translator\n",
    "import random\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap to Text Mining\n",
    "\n",
    "Text mining est le processus d'extraction d'informations à partir de grandes quantités de données non structurées. Cela implique plusieurs étapes, qui peuvent être décomposées en ces étapes suivantes :\n",
    "\n",
    "1. Collecte des données : La première étape du text mining est la collecte de grande quantité de données non structurées. Cette donnée peut provenir de diverses sources telles que les plateformes sociaux, les sites Web, les blogs et les articles de presse.\n",
    "\n",
    "2. Prétraitement : Une fois les données recueillies, elles doivent être prétraitées. Cela consiste à nettoyer les données en supprimant les caractères indésirables ou les mots, en corrigeant les erreurs orthographiques et en convertissant le texte en minuscule.\n",
    "\n",
    "3. Tokenisation : Une fois les données prétraitées, elles sont tokenisées. Cela signifie que le texte est divisé en mots individuels ou des tokens. Cette étape est importante car elle permet d'analyser la fréquence de chaque mot dans le texte.\n",
    "\n",
    "4. Suppression des mots-clés : Une fois le texte tokenisé, nous pouvons supprimer les mots-clés. Les mots-clés sont des mots courants tels que \"le\", \"et\" et \"est\" qui n'apportent pas de sens particulier et peuvent être supprimés sans affecter l'analyse globale.\n",
    "\n",
    "5. Reduction morphologique ou normalisation du lemme : La prochaine étape est la reduction morphologique ou la normalisation du lemme. Cela consiste à réduire chaque mot à sa forme de base, également connu sous le nom de racine de mot. Par exemple, \"courir\" serait réduit à \"court\". Cela est fait pour regrouper des mots similaires ensemble, comme différents temps verbaux du même mot.\n",
    "\n",
    "6. Extraction des fonctionnalités : Une fois le texte prétraité et nettoyé, nous pouvons extraire les fonctionnalités à partir de lui. Ces fonctionnalités peuvent inclure des choses telles que la fréquence des mots, l'analyse du sentiment ou le modélisation des sujets.\n",
    "\n",
    "7. Analyse : Enfin, nous pouvons analyser les données à l'aide de diverses techniques telles que l'agrégation ou la classification pour obtenir des informations sur le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "FIGSIZE = (12, 4)\n",
    "PATH_DATA = \"../data/\"\n",
    "PATH_RAW = PATH_DATA + \"raw/\"\n",
    "PATH_PROCESSED = PATH_DATA + \"processed/\"\n",
    "PATH_EXTERNAL = PATH_DATA + \"external/\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN  3804725264  1263597046  \n",
       "1                                                NaN   436067568  1008141237  \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978  \n",
       "3                                                NaN    50418756   457047496  \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84916</th>\n",
       "      <td>Folkmanis Puppets - 2732 - Marionnette Et Théâ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84917</th>\n",
       "      <td>Porte Flamme Gaxix - Flamebringer Gaxix - 136/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84918</th>\n",
       "      <td>Pompe de filtration Speck Badu 95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84919</th>\n",
       "      <td>Robot de piscine électrique</td>\n",
       "      <td>&lt;p&gt;Ce robot de piscine d&amp;#39;un design innovan...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84920</th>\n",
       "      <td>Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "84916  Folkmanis Puppets - 2732 - Marionnette Et Théâ...   \n",
       "84917  Porte Flamme Gaxix - Flamebringer Gaxix - 136/...   \n",
       "84918                  Pompe de filtration Speck Badu 95   \n",
       "84919                        Robot de piscine électrique   \n",
       "84920  Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "84916                                                NaN   516376098   \n",
       "84917                                                NaN   133389013   \n",
       "84918                                                NaN  4128438366   \n",
       "84919  <p>Ce robot de piscine d&#39;un design innovan...  3929899732   \n",
       "84920                                                NaN   152993898   \n",
       "\n",
       "          imageid  \n",
       "84916  1019294171  \n",
       "84917  1274228667  \n",
       "84918  1295960357  \n",
       "84919  1265224052  \n",
       "84920   940543690  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1773.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>788.179885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1281.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2905.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prdtypecode\n",
       "count  84916.000000\n",
       "mean    1773.219900\n",
       "std      788.179885\n",
       "min       10.000000\n",
       "25%     1281.000000\n",
       "50%     1920.000000\n",
       "75%     2522.000000\n",
       "max     2905.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = pd.read_csv(PATH_RAW + \"x_train.csv\", index_col=0)\n",
    "X_test = pd.read_csv(PATH_RAW + \"x_test.csv\", index_col=0)\n",
    "display(X_train.head())\n",
    "display(X_test.head())\n",
    "\n",
    "target = pd.read_csv(PATH_RAW + \"y_train.csv\", index_col=0)\n",
    "\n",
    "display(target.describe())\n",
    "df_origin = pd.read_csv(PATH_RAW + \"x_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 1)\n",
      "498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'à', 'â', 'abord', 'afin', 'ah', 'ai', 'aie', 'ainsi', 'allaient']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_french = pd.read_json(PATH_EXTERNAL + \"stop_words_french.json\")\n",
    "print(stop_words_french.shape)\n",
    "stop_words = []\n",
    "stop_words.extend(stop_words_french[0].tolist())\n",
    "stop_words.extend([\"cm\", \"mm\"])\n",
    "print(len(stop_words))\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING LEXIQUE FRANÇAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(texts):\n",
    "    sentences = []\n",
    "    for sentence in tqdm(texts):\n",
    "        if sentence is np.nan:\n",
    "            sentences.append(np.nan)\n",
    "        else:\n",
    "            s = \"\".join(\n",
    "                c\n",
    "                for c in unicodedata.normalize(\"NFD\", sentence)\n",
    "                if unicodedata.category(c) != \"Mn\"\n",
    "            )\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ortho</th>\n",
       "      <th>lemme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a capella</td>\n",
       "      <td>a capella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a cappella</td>\n",
       "      <td>a cappella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a contrario</td>\n",
       "      <td>a contrario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a fortiori</td>\n",
       "      <td>a fortiori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142685</th>\n",
       "      <td>ôtèrent</td>\n",
       "      <td>ôter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142686</th>\n",
       "      <td>ôté</td>\n",
       "      <td>ôter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142688</th>\n",
       "      <td>ôtée</td>\n",
       "      <td>ôter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142690</th>\n",
       "      <td>ôtées</td>\n",
       "      <td>ôter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142692</th>\n",
       "      <td>ôtés</td>\n",
       "      <td>ôter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125652 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ortho        lemme\n",
       "0                 a            a\n",
       "3         a capella    a capella\n",
       "4        a cappella   a cappella\n",
       "5       a contrario  a contrario\n",
       "6        a fortiori   a fortiori\n",
       "...             ...          ...\n",
       "142685      ôtèrent         ôter\n",
       "142686          ôté         ôter\n",
       "142688         ôtée         ôter\n",
       "142690        ôtées         ôter\n",
       "142692         ôtés         ôter\n",
       "\n",
       "[125652 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexique = pd.read_table(PATH_EXTERNAL + \"Lexique383/Lexique383.tsv\", delimiter=\"\\t\")\n",
    "lexique_filtred_unique = lexique.drop_duplicates(subset=\"ortho\")[[\"ortho\", \"lemme\"]]\n",
    "lexique_filtred_unique.dropna(subset=[\"ortho\", \"lemme\"], axis=\"index\", inplace=True)\n",
    "# lexique_filtred_unique[\"ortho\"] = strip_accents(lexique_filtred_unique[\"ortho\"])\n",
    "# lexique_filtred_unique[\"lemme\"] = strip_accents(lexique_filtred_unique[\"lemme\"])\n",
    "lexique_filtred_unique = lexique_filtred_unique.drop_duplicates(subset=\"ortho\")[\n",
    "    [\"ortho\", \"lemme\"]\n",
    "]\n",
    "\n",
    "lexique_filtred_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premier nettoyage\n",
    "### pour pouvoir traduire les textes d'une autre langue en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_cleaning(sentences):\n",
    "    # Pre-compile regex patterns\n",
    "    HTML_TAGS_RE = re.compile(\"<[^>]*\")\n",
    "    URL_RE = re.compile(r\"https?://[-_.?&~;+=/#0-9A-Za-z]{1,2076}\")\n",
    "    MAIL_RE = re.compile(\n",
    "        r\"[-_.0-9A-Za-z]{1,64}@[-_0-9A-Za-z]{1,255}[-_.0-9A-Za-z]{1,255}\"\n",
    "    )\n",
    "    SPE_CHAR_RE = re.compile(\"[^a-zA-ZÀ-ÿ]\")\n",
    "    SPACES_RE = re.compile(\"\\s+\")\n",
    "    SPACES_RE_2 = re.compile(\" +\")\n",
    "\n",
    "    sentences_cleaned = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Decode HTML entities\n",
    "        sentence = html.unescape(sentence)\n",
    "\n",
    "        # Replace HTML tags with spaces\n",
    "        sentence = HTML_TAGS_RE.sub(\"\", sentence)\n",
    "\n",
    "        # Remove URL and e-mail adress\n",
    "        sentence = URL_RE.sub(\"\", sentence)\n",
    "        sentence = MAIL_RE.sub(\"\", sentence)\n",
    "\n",
    "        # Replace special characters with spaces while keeping accents\n",
    "        sentence = SPE_CHAR_RE.sub(\" \", sentence)\n",
    "\n",
    "        # Remove unnecessary spaces from the sentence using regular expressions\n",
    "        cleaned_sentence = SPACES_RE.sub(\" \", sentence)\n",
    "        cleaned_sentence = SPACES_RE_2.sub(\" \", cleaned_sentence)\n",
    "        sentences_cleaned.append(cleaned_sentence)\n",
    "\n",
    "    return sentences_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"designation\"] = first_cleaning(X_train[\"designation\"])\n",
    "X_train[\"description\"].loc[~X_train[\"description\"].isna()] = first_cleaning(\n",
    "    X_train[\"description\"].loc[~X_train[\"description\"].isna()]\n",
    ")\n",
    "\n",
    "# join texts\n",
    "X_train[\"text\"] = np.where(\n",
    "    X_train[\"description\"].isna(),\n",
    "    X_train[\"designation\"].astype(str),\n",
    "    X_train[\"designation\"].astype(str) + \" \" + X_train[\"description\"].astype(str),\n",
    ")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7f4a1e8133446183535c9a65e6c588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07ce6eed5814c5a9cf149ebd936f46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/4077928039.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"description\"].loc[~X_test[\"description\"].isna()] = first_cleaning(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84916</th>\n",
       "      <td>Folkmanis Puppets Marionnette Et Théâtre Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "      <td>Folkmanis Puppets Marionnette Et Théâtre Mini ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84917</th>\n",
       "      <td>Porte Flamme Gaxix Flamebringer Gaxix U Twilig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "      <td>Porte Flamme Gaxix Flamebringer Gaxix U Twilig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84918</th>\n",
       "      <td>Pompe de filtration Speck Badu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "      <td>Pompe de filtration Speck Badu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84919</th>\n",
       "      <td>Robot de piscine électrique</td>\n",
       "      <td>Ce robot de piscine d un design innovant et é...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "      <td>Robot de piscine électrique  Ce robot de pisci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84920</th>\n",
       "      <td>Hsm Destructeur Securio C Coupe Crois E X Mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "      <td>Hsm Destructeur Securio C Coupe Crois E X Mm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "84916  Folkmanis Puppets Marionnette Et Théâtre Mini ...   \n",
       "84917  Porte Flamme Gaxix Flamebringer Gaxix U Twilig...   \n",
       "84918                    Pompe de filtration Speck Badu    \n",
       "84919                        Robot de piscine électrique   \n",
       "84920       Hsm Destructeur Securio C Coupe Crois E X Mm   \n",
       "\n",
       "                                             description   productid  \\\n",
       "84916                                                NaN   516376098   \n",
       "84917                                                NaN   133389013   \n",
       "84918                                                NaN  4128438366   \n",
       "84919   Ce robot de piscine d un design innovant et é...  3929899732   \n",
       "84920                                                NaN   152993898   \n",
       "\n",
       "          imageid                                               text  \n",
       "84916  1019294171  Folkmanis Puppets Marionnette Et Théâtre Mini ...  \n",
       "84917  1274228667  Porte Flamme Gaxix Flamebringer Gaxix U Twilig...  \n",
       "84918  1295960357                    Pompe de filtration Speck Badu   \n",
       "84919  1265224052  Robot de piscine électrique  Ce robot de pisci...  \n",
       "84920   940543690       Hsm Destructeur Securio C Coupe Crois E X Mm  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[\"designation\"] = first_cleaning(X_test[\"designation\"])\n",
    "X_test[\"description\"].loc[~X_test[\"description\"].isna()] = first_cleaning(\n",
    "    X_test[\"description\"].loc[~X_test[\"description\"].isna()]\n",
    ")\n",
    "\n",
    "# join texts\n",
    "X_test[\"text\"] = np.where(\n",
    "    X_test[\"description\"].isna(),\n",
    "    X_test[\"designation\"].astype(str),\n",
    "    X_test[\"designation\"].astype(str) + \" \" + X_test[\"description\"].astype(str),\n",
    ")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection des langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(sentences):\n",
    "\n",
    "    UNKNOWN = \"unknown\"\n",
    "    FR = \"fr\"\n",
    "\n",
    "    languages = []\n",
    "    k = 15\n",
    "    lexique_set = set(lexique_filtred_unique[\"ortho\"].str.strip().str.lower())\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Split the sentence into words\n",
    "        words = nltk.word_tokenize(sentence, language=\"french\")\n",
    "\n",
    "        # If there are more than k words, sample k words randomly.\n",
    "        if len(words) > k:\n",
    "            words = random.sample(words, k)\n",
    "\n",
    "        # Use a list comprehension with a ternary conditional operator for language detection.\n",
    "        df_la = []\n",
    "        for word in words:\n",
    "            if word.lower() in lexique_set:\n",
    "                df_la.append(FR)\n",
    "            else:\n",
    "                try:\n",
    "                    df_la.append(detect(word))\n",
    "                except LangDetectException:\n",
    "                    df_la.append(UNKNOWN)\n",
    "\n",
    "        # Use Counter to find the most common language.\n",
    "        reel_lang = Counter(df_la).most_common(1)[0][0] if len(df_la) > 0 else UNKNOWN\n",
    "        languages.append(reel_lang)\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"language\"] = detect_language(X_train[\"text\"])\n",
    "X_train.to_csv(PATH_PROCESSED + \"X_train_detected_language.csv\")\n",
    "display(X_train.head())\n",
    "X_train[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77abd4834629470080bf81def552e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84916</th>\n",
       "      <td>Folkmanis Puppets Marionnette Et Théâtre Mini ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "      <td>Folkmanis Puppets Marionnette Et Théâtre Mini ...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84917</th>\n",
       "      <td>Porte Flamme Gaxix Flamebringer Gaxix U Twilig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "      <td>Porte Flamme Gaxix Flamebringer Gaxix U Twilig...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84918</th>\n",
       "      <td>Pompe de filtration Speck Badu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "      <td>Pompe de filtration Speck Badu</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84919</th>\n",
       "      <td>Robot de piscine électrique</td>\n",
       "      <td>Ce robot de piscine d un design innovant et é...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "      <td>Robot de piscine électrique  Ce robot de pisci...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84920</th>\n",
       "      <td>Hsm Destructeur Securio C Coupe Crois E X Mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "      <td>Hsm Destructeur Securio C Coupe Crois E X Mm</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "84916  Folkmanis Puppets Marionnette Et Théâtre Mini ...   \n",
       "84917  Porte Flamme Gaxix Flamebringer Gaxix U Twilig...   \n",
       "84918                    Pompe de filtration Speck Badu    \n",
       "84919                        Robot de piscine électrique   \n",
       "84920       Hsm Destructeur Securio C Coupe Crois E X Mm   \n",
       "\n",
       "                                             description   productid  \\\n",
       "84916                                                NaN   516376098   \n",
       "84917                                                NaN   133389013   \n",
       "84918                                                NaN  4128438366   \n",
       "84919   Ce robot de piscine d un design innovant et é...  3929899732   \n",
       "84920                                                NaN   152993898   \n",
       "\n",
       "          imageid                                               text language  \n",
       "84916  1019294171  Folkmanis Puppets Marionnette Et Théâtre Mini ...       fr  \n",
       "84917  1274228667  Porte Flamme Gaxix Flamebringer Gaxix U Twilig...       fr  \n",
       "84918  1295960357                    Pompe de filtration Speck Badu        fr  \n",
       "84919  1265224052  Robot de piscine électrique  Ce robot de pisci...       fr  \n",
       "84920   940543690       Hsm Destructeur Securio C Coupe Crois E X Mm       fr  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "language\n",
       "fr    13124\n",
       "en      295\n",
       "de      113\n",
       "tl       29\n",
       "it       28\n",
       "ro       25\n",
       "fi       21\n",
       "so       20\n",
       "sl       19\n",
       "id       17\n",
       "pt       13\n",
       "pl       12\n",
       "no       11\n",
       "es       11\n",
       "sw       10\n",
       "af        9\n",
       "nl        9\n",
       "ca        8\n",
       "cs        7\n",
       "cy        7\n",
       "da        6\n",
       "lt        6\n",
       "sv        3\n",
       "tr        3\n",
       "sq        2\n",
       "hu        1\n",
       "et        1\n",
       "vi        1\n",
       "sk        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[\"language\"] = detect_language(X_test[\"text\"])\n",
    "X_test.to_csv(PATH_PROCESSED + \"X_test_detected_language.csv\")\n",
    "display(X_test.head())\n",
    "X_test[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traduction en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(translator: google_translator, sentences, language):\n",
    "    texts_translated = translator.translate(sentences, lang_tgt=\"fr\", lang_src=language)\n",
    "    return texts_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(PATH_PROCESSED + \"X_train_detected_language.csv\", index_col=0)\n",
    "\n",
    "# si error durant la traduction -> reprendre à l'index done\n",
    "done = 16\n",
    "\n",
    "# google translate API\n",
    "translator = google_translator(timeout=10000000)\n",
    "\n",
    "X_train_copy = X_train.copy()\n",
    "j = 0\n",
    "for language in tqdm(X_train_copy[\"language\"].value_counts().index):\n",
    "    j += 1\n",
    "    if j > done and language != \"fr\":\n",
    "        texts_translated = []\n",
    "        df_lang = X_train_copy[\"text\"].loc[X_train[\"language\"] == language].tolist()\n",
    "\n",
    "        for i in tqdm(range(0, len(df_lang)), desc=language):\n",
    "            texts_translated.append(translate_texts(translator, df_lang[i], language))\n",
    "        print(texts_translated[:10])\n",
    "        X_train_copy[\"text\"].loc[X_train[\"language\"] == language] = texts_translated\n",
    "        # save\n",
    "        X_train_copy.to_csv(PATH_PROCESSED + \"X_train_translated.csv\")\n",
    "\n",
    "X_train = pd.read_csv(PATH_PROCESSED + \"X_train_translated.csv\", index_col=0)\n",
    "X_train[\"text\"].loc[X_train[\"language\"] == \"de\"]\n",
    "\n",
    "print(\"is NaN :\")\n",
    "indexes = X_train[X_train[\"text\"].isna()].index\n",
    "df_origin.iloc[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea0c1877e2409bbff3b4893c8ef738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f691a4ad8d484f2ab8ddd1eb89814db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en:   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Xbox One à l'édition Call of Duty Advanced Warfare \", 'Kinect Joy Ride Jeu Kinect Jeu Xbox ', 'Annuaire de conception néerlandaise ', 'Enfants Soft Plux Doll Simulation mignon Cat Artisanat Toy Car Decoration GiftZen Enfants Soft Plux Dol Doltesimulation Cat Crafts Toy Car Decoration Cadeau Gifts Les décorations ou les cadeaux de vacances Cat est suffisant pour que vous vous sentiez mieux. ', 'Bouteille de vins Sacs de couverture Décoration Home Party Santa Claus Christmashpp Christmas Gift Sac Sac Candy Merry Christmas Sacs de bonbons de Noël décorféature de haute qualité PC MATÉRIAU Arbres de Noël ou décoration de lieu et ainsi de suite contenu du paquet x sac cadeau de Noël sac de bonbons joyeux Noël sacs de bonbons décor de Noël sans pomme ', 'Les Berenstain Bears visitent le dentiste ', 'Le jeu de chenilles très affamé PA Game de comptage des couleurs et des contrastes P ', \"Couleur métallique de luxe Skin étanche PVC Stickers pour DJI Osmo Pocketzen Générique Couleur métallique de luxe Skin Skinproof PVC Stickers pour DJI Osmo PocketFeAture Couleur Luxury Show Off the Color Beauty Osmo Pocket Stickers Agit Pocketwater Proofscrach Proofsun Proof avec des emplacements d'air plus facile à gérer avec des bulles pratiques pour plaire à coller facile à coller et à déchirer les offsticles recouverts de films fait que les motifs des conceptions plus magnifiques changent arbitrairement les effectifs incluent des autocollants PC Osmo Pocket Drone n'est pas inclus n'est pas inclus \", \"D Peinture Fee Papillon Bricolage Diamant Broderie Point de Croix Activeil DÉCORATIONS LA D D FAIRY BUTPULT LIY Point de forage cm pouce cm pouces chambre salon chambre à coucher salle d'étude entrée bricolage des étapes de production de peinture de diamant ouvrez la boîte et cochez le diamant des outils spéciaux Affichage de la couleur de diamant en résine disposée dans l'ordre de codage du dessin de ruban décontracté ci-dessus, vous verrez beaucoup de symboles correspondant au codage couleur en fonction de la pince codée en couleur correspondante, les diamants correspondants en résine incrustés suggèrent qu'un type de diamant en résine un ensemble terminé plus rapidement afin de créer des dessins de peinture de diamant parfaits assemblés en un seul endroit à chaque symbole de ligne n'a pas à à Restez collé aux diamants pour couper une bonne silhouette sur les dessins maintient le tableau trié monté sur le plan du matériau au-dessus des épissures nécessite un plat contre la pure n'a pas de fissures après un bon combat le reste de l'espace au niveau de la colle correspondant aux symboles des diamants Un bon dessin en diamant le place dans votre sélection d'un cadre approprié Le restaurant ne fournit pas de cadre de contenu de package X Fairy Butterfly Paint \", 'Persona Chogokin Aegis PVC Figurine ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52b75b48af4950a6d103b412a0a936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liberté individuelle et collective en droit du travail ', 'Amano du Soleil Amano Herald of the Sun U Trône des marées ', 'Moment de ma vie ', 'Spark Hill GH GP Italie S Spark Fabricant Spark Echelle Ref Fab S Type Hill GH GP Italie Couleur Blanc Rouge Ean Ref Little Bolide ', 'Locomotive ge ton commutateur sud du Pacifique REC Ech Bachmann Bachmann ', 'King Richard Shakespeare Peter Ure Methuen ', 'Bitz Warhammer Fenêtre ', 'Moshi Monsters Glitter and Glow Moshi Monsters brille et brille ', 'Sac de couchage töllner confortable ', 'Oui ma fille chérie ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c522e0671a543369890413bed738c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tl:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PS Slim Go GT ', \"Génération de puissance conventionnelle et alternative à l'atténuation et à la durabilité de la thermodynamique \", 'Catalogue Casio Calculatrices Micro Ornineurs PB P P P P P P P ', 'Easy le fait pour la phonologie ', 'Classic Tales Magic Cook Pot AB ED ', 'Nobunaga no yabou super famicom ', \"Journal de Pitman n du l'étudiant des notes de conservation des notes et des nouvelles de l'enseignement des nouvelles de la banque inférieure AMALGAMATION \", 'Colgante Douard Dangly Elefante Playgro M ', 'Magic Grow Animaux de safari ', 'Débutants de Big Ben ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b5d12739f646f0bf318fdc0de1e13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "it:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pontiac GTO juge Orange Motormax Motormax Pontiac GTO juge Orange Motormax ', \"Alliances d'armure viscérides VO \", \"Il est temps du volume musical recueil est le temps de la musique est une méthode destinée aux jeunes étudiants qui entreprennent le chemin musical est divisé en trois volumes organisés en unités dans chacune des exercices théoriques théoriques avec des résumés et des tests de vérification des applications pratiques sont traités pour parler à une et deux voix et rythmique à une et deux parties, une attention particulière est accordée avec de courtes notes à la fin de chacune des unités qui composent les volumes dans les sujets suivants le volume de volume des instruments de musique Pour la systématicité de la méthode et pour son contenu inspiré par le nouveau programdidactique, le travail est en particulier aux élèves du collège avec une adresse musicale et en général à tous ceux qui veulent entreprendre l'étude de la musique \", 'Lola t norev ', 'Valentino Rossi le jeu ', 'Castillo ou Jorge Galindo Elixir ', 'Renault Clio V ème Norev ', 'Suivant dans les conseils et conseils de surintendance ', 'Call of Duty MW Edition Collector durcised ps ', \"Durer et le catalogue italien de l'exposition de mars de Rome en mars \"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a7b95ca72c4782a9206aab83f1696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ro:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mug ML Scar Call of Duty Infini Warfare ', 'Arabes manuscritos del Lebano ', 'Spacemaster Future Law ', 'Livrets Lindner R Belgique Supplement Année de cisaillement Numéro H Nombre de pages ', 'Ty beanie bébé chanceux la coccinelle pty beanie bébés chanceux la coccinelle ', \"Acier en acier inoxydable à thé épice d'épices à l'infuseur Filtre de file du maillage de maille avec couvercle Chainhpp en acier inoxydable Balle de thé à thé Infuseur Filtre de maillage de maille avec couvercle Chandescriptif matériau inoxydable STEE SLIVER TAILLE M CMNOTE MADÉE par acier inoxydable est non toxic Les feuilles de thé Pawan peuvent être retirées du thé étranger peut être recyclé, comprend des épices de thé PCS à l'étranger \", 'Heures le mans ixo ', 'Jeu de cartes à collectionner naruto naruto uzumaki pr ', 'Torune ninja dans le livre Naruto Shippuden VF ', 'Microfluidics et nanofluidiques motivés électrokinétiquement ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcb3839b14944049d300e1a0d9fdf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fi:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maito Chevrolet Camaro SS RS Police Pol Topta ', 'Fallout Vault Swimshort S ', 'Armure plus un camion ', 'Figurine Lucky Luke Dalton Avell Happy Meal McDo ', 'CONSES BI DI GUO ZHU YI LUN ', 'Noir Blanc Feuiilajou Feuiloutan Flamajou Flamoutan Flotajou Flotoutan ', 'Glizer Polaris Blister Glizer Polaris CM Honeym ', 'Karasuno Aobajohsai Nekoma Fukurodani Goodies Havyu ', 'Jikkyou puissant pro yakyuuu ', \"Boku wa crochets pour le pont de l'importation ds japonisis \"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbfeebc4c85451ab8e6e8d7c0b63445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "so:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mynock dagobah star wars ccg ls ', 'Cadilles de luxe Cadilla Crossover Collectbibles de luxe ', 'Xbox Go ', 'Boit la fièvre umd vidéo ', 'Microsoft Crackdown Xbox One Bassic Xboog One VideogiococococoCo Crackdown Xboosoft Xboosoft Xboo Jebox ', 'Xbox Kinect Noir ', 'Glom Hs Undefted ', 'Jedi Lévitation Dagobah Star Wars CCG ', 'Star Wars Bust Ups Clone Wars Padmala Amidala ', 'Dark Horse Comics ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61996fd584244c4cba3bd64ac19e1be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sl:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tele K Rupert Everett P X Files le Movie P Kim Basinger P Sol Si Zazie Catherine Lara Cabrel Jonasz P Raphael Dinelli Pete Goss Voile P Anna Falchi P Kurt Russell P Didier Bourdon Bernard Campan P ', 'T J Sophie Marceou P Jeanne Moreeu P Bernard Rapp P Hulot P Hulot P Hulot Lanoux Marie Jose Nat P Richard Chamberlain P Marie Antoinette P Anny Duperey Bernard Giraudeau P Dorothee P Coppens P ', 'Slime métallique ultime ptdn en ', 'Warning: Can only detect less than 5000 characters', 'Zembla n Lug ', 'Entiné ', 'Sheila P Barbra Streisand Film Hello Dolly P Mireille Mathieu P Charles Aznavour P Nana Mouskouri P Brigitte Bardot P ', 'Boosters Pokémon SL ', 'VSD n Thierry Ardisson P David Hallyday P Dick Rivers P ', 'Hook Megadrive FR Rate Megadrive ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffad38ea4874fad8343468907f9ff97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "id:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Arbre d'essieu arrière en titane pour Ball R Japan Importation \", 'Paraguas CMS Automático Baggy Aguacero Paraguas CMS Automático Baggy Aguacero ', 'Salamandra Ultra Monster Japan Importation ', 'Bruna Kimono Wedding Doll Japan Import ', 'Cyberdimension Neptunia Goddess en ligne ', 'PROFAIN PERFECT ADDON NURNBERG SAALFELD ', 'Monolithe de basalte révisé VO ', 'Tantam d le métier ', 'Ultraman super-héros de la série Ultra Hero Ultraman Astra Japan Importation ', 'Kamen Rider Blade Ross Emission Shadow Chaser Japan Import ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2506f12ddfe4fc6ae2069821feafbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pt:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Remooraid PV ', 'Promouvoir Ferrari Modena Coupé Blister Pro Promotion ', 'Ortide Édition Ed ', \"Forme de Dieu forme d'un serviteur \", 'Gundam HCM Pro Riser ', 'Rivaux Emergeans X Nidoran Nidorina Nidorino ', 'Nendoroid Annegasaki Nene Pvc Costurine ', 'Gundam sd croix os gundam x ', 'Armodon Dressé Formé Armodon Magic Mtg Tempète C ', 'Gundam x Gundam Ashtaron Scale ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e8bad1dc2245cc83a9a46f164011d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pl:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Manches en silicone en acier inoxydable Paies de consommation d'alcool pour oz tasses Oz Orhpp Silicone Gouetter Pour nettoyer la longueur de taille mm diamètre mmpackage teneur en acier inoxydable paille en acier inoxydable \", 'Sony Playstation Slim Go ', 'Sony PS Vita Noir ', 'Carte WWE Slam Attax Todd Grisham Raw ', 'Brosse à dents parlante chan après après chii votre outil après Chan Japan Importation ', 'Attaque spyro des Rhynocs ', 'Sony Playstation Slim Go ', 'Kid klown en foulard fou ', 'Zoids Grande-Bretagne GB Bio Ptera Scale ', 'Sony Playstation Slim Go ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa568bf08f6a4eaaa6fcd6e5252aeb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "no:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vds res ', 'Norev Renault Jetcar Noreev ', 'Oxford Manuel de médecine aiguë ', 'Harbinger ', 'Noreev Peugeot t Noreev ', 'Inuyasha naraku no wana mayoi no mori no shoutaijou ', 'La légende de Zelda Zelda no densetsu fushigi no boshi minish capride version japonaise ', 'Héroclix Council Token Avengers Everett K Ross Avengers B ', 'HARRY POTTER PULLOVER HOODIE Girl Poufsouffle XL ', 'Non à la vache ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdf51fa06524c7e810d8a5bbe842b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "es:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mercedes Unimog Pompiers et solide ', \"Piscine TOI Ethnique X x cm Piscine amovible TOI ethnique cm cm long cm large et cm de haut le complément de votre jardin avec lequel vous obtiendrez une augmentation du confort et de la qualité de vie de cet été fabriqué dans des matériaux résistants aux murs en acier laquée Fermeture en acier avec double vis à double vis Rangée et recouverte d'un boîtier en polyéthylène décoratif de haute densité et de qualité photographique, le filtre de bain d'été le plus rafraîchissant et le plus sûr, volez l'escalier décoratif de décoce de décoce et de la série de décoration de la piscine Traité en PVC, vous pouvez renouveler votre système de décoration de pool de décoco, vous pouvez renouveler votre Pool mural en acier TOI TOI Chaque année, les couvertures du système de piscine DeCoh sont fabriquées en polyéthyle haute densité et qualité photographique créée dans des matériaux biodégradables avec l'environnement le moyen économique et le plus simple le plus simple de modifier l'apparence extérieure de votre nouvelle piscine \", 'Sketch RV et sermons Bible Éphésiens Éphésiens Colossiens ', 'Lefebure Francis expérimente les initiatiques tomes ', 'Mercedes Sl Coupé N solide ', \"Calendrier de l'âme \", 'Ferrari GTB Bburago ', 'Expicot Astrub Carte Wakfu TCG ', 'Racines anciennes de nombreuses branches ', 'Gauge aérodynamique Nintendo ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90445f1ae6db48efba2a477a80170b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sw:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Masha et Michka Activites Avec Masha ', 'MP Fr Espoir Barian Cxyz ', 'Ichiban Assassinat Assassination Classroom Peluche Koro Sensei ', 'Momification dédiée Amakna ', 'Bokujou Monogatari Mineral Town No Nakama Tachi ', 'Jeu Call of Duty MW Wii ', 'Carte dédiée Yokaï, édition Firefoux Pandala ', 'Wii u édition Zelda Wind Waker Jeuux ', 'San Ku Kai Robot Sidero Popy Japan ', 'Aussi Carrot Sayaka Takai Swimsuit Ver PVC Figure Scale ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a913e3b27f48f3a8a9a73e3cd11727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "af:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Notebook Journal Journal grand cahier liné pour tous les projets moutarde ', 'Personnages Disney Crystalux Rapunzel CM ', \"Jewels de l'île tropicale perdue \", 'Overwatch Legendary Edition ', 'Walking Dead Mug Maggie ', 'Mon cahier carnet en blanc ', 'Toys Dinky Meccano Paquebot La Normandie Dinky Toys ', 'Monde magique Disney ', 'DK Eyewitness Travel Guide London ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97863d69e84c21b3b182f306185c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nl:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fleut detlef schrempf n ', 'Devanar Damir Wars TCG déchiffrer ', 'Fleu Clifford Robinson N ', 'Opel Ascona S R I Yellow White Neo Limited PCS Gelb Weiss Neo ', \"Pin d'articulation degrés degrés FRX Pin de charnière Brace deg \", 'Lame de la reine Shizuka Megahouse ', 'Volcaropode de Limagma néo ', 'Ferrari FXX Yellow Hotheels Elite Mattel Hot Wheels ', \"Avancement de la physiopathologie de l'AVC cérébral \"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1913f036b4a42b4813b964420240826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ca:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ex Libris Magica R ', 'Mortel kombat mug klassic ', 'Emerald Dragonfly Magic MTG Chronicles C ', 'Ecran tactile inférieur nintendo ds ', 'Duaigües JARGE J Actividad physique pour les personnages avec disque ', 'Aqua Kitty DX Limited Run ', 'Exercice pour les os forts ', 'Pokénav Trainer PV EMERADE FEE ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc461cfb51544455bd88e2df43ecfec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pokémon R Rondoud Reverse ', 'Pokémon R ColimUcus inverse ', 'Pokémon Edition Blanca ', 'Pokémon R Electrode Niv Inverse ', 'Pokémon R Milobellus Inverse ', 'Par Désir Project Toyosakomimino Miko Scale ', 'Pokémon Makuhita NIV PV ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b651cea89a48b5acf74a2b4511b560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cy:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boyds Grannie Annie Wishkabibble par Boyds Bears Boyds Grannie Annie Wishkabble par Boyds Bears ', 'Farcry xbox ', 'BYCMO Subaru Impreza Tunning Edition ', 'Dawn of War Winter Assault Ajouter sur Du Jeu Dawn of War ', 'Japon Import Turtle Purdle ', 'Soyez sage mon fils et rendez mon cœur heureux ', 'Molloy de Samuel Beckett ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c49063674eb4417989c8ea03f16ad1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "da:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eternal Champions Mega CD ', 'Magnét européen valladolid ', 'Vampire la lutte éternelle Agrippina Jyhad Vo ', 'Skylanders Giants Fright Rider ', 'Tiger Woods PGA Tour ', 'Outils créatifs crochets magnétiques à cochons de casier scolaire réfrigérateur faim bluehpp outils créatifs crochets magnétiques cordons de casier scolaire réfrigérateur faim blue dispose de toute marque et de haute qualité en acier inoxydable en plastique magnétique durable et de longue date Couleur Bluemax Roueur kg taille CMPackage comprend x crochet ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a8071ee9714b049e4b0414be3af61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lt:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carburant Tenseei Eme Cercle ', 'Upsie Downsie êtes-vous asep ', 'Lamborghini Miura Sv Rouge Italia Rosso Red Deagostini Deagostini ', 'Gravity Rush PS uniquement ', 'Mariokart Jeu DS Nintendo ', 'Painiac mtg instable vo c ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f167e57a3c264665a86b8a833979b988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sv:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MUSTEFLOTT HOLO NIV RIVUX EMERSIANTS PV ', 'Harn nyl régulation ora ', 'Hasbro Nerf Elite Rayven ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aa4843ed7144698ed17d71c4263c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tr:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Construisez-le Miami Beach Resort ', 'Lindner Pologne Supplement Year Numéro Nombre de pages ', 'Hitman Import UK ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f233b73e4c7e4df3bc86abf9d9d49c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sq:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shin Megami Tensei personne ', 'LOTR CCG úLAIRE NERTAA HUNTER ENTÉ ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaed9ee73f004bf0bb344be058e46ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hu:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVOVE Edition Benelux ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbd432f3ab34031b57fc1ad758b07ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "et:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solino Mitsubishi Pajero MPR Mitsubishi ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe1323f9dbf474ba8a58396b28a9bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vi:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CV Tourring Belge ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6392ec39e01e49f590613ff7a6051bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sk:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Robo force ennemi le dictateur ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/rfghqg0j31d_t9y5_yrj2ss00000gn/T/ipykernel_52227/1665405426.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is NaN :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [designation, description, productid, imageid]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(PATH_PROCESSED + \"X_test_detected_language.csv\", index_col=0)\n",
    "\n",
    "# si error durant la traduction -> reprendre à l'index done\n",
    "done = 0\n",
    "\n",
    "# google translate API\n",
    "translator = google_translator(timeout=10000000)\n",
    "\n",
    "X_test_copy = X_test.copy()\n",
    "j = 0\n",
    "for language in tqdm(X_test_copy[\"language\"].value_counts().index):\n",
    "    j += 1\n",
    "    if j > done and language != \"fr\":\n",
    "        texts_translated = []\n",
    "        df_lang = X_test_copy[\"text\"].loc[X_test[\"language\"] == language].tolist()\n",
    "\n",
    "        for i in tqdm(range(0, len(df_lang)), desc=language):\n",
    "            texts_translated.append(translate_texts(translator, df_lang[i], language))\n",
    "        print(texts_translated[:10])\n",
    "        X_test_copy[\"text\"].loc[X_test[\"language\"] == language] = texts_translated\n",
    "        # save\n",
    "        X_test_copy.to_csv(PATH_PROCESSED + \"X_test_translated.csv\")\n",
    "\n",
    "X_test = pd.read_csv(PATH_PROCESSED + \"X_test_translated.csv\", index_col=0)\n",
    "X_test[\"text\"].loc[X_test[\"language\"] == \"de\"]\n",
    "\n",
    "print(\"is NaN :\")\n",
    "indexes = X_test[X_test[\"text\"].isna()].index\n",
    "df_origin.iloc[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text) -> list:\n",
    "    \"\"\"\n",
    "    This function removes stop words from a given text and returns a list of sentences without stop words. It also removes unnecessary spaces, converts all characters to lowercase, and trims leading and trailing spaces.\n",
    "\n",
    "    Parameters:\n",
    "        text (str or list): The input text can be either a string containing multiple sentences separated by newline characters ('\\n') or a list of strings representing individual sentences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentences without stop words and other modifications as described above.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty list for sentences\n",
    "    sentences = []\n",
    "    lemmes = []\n",
    "\n",
    "    # Pre-compile regex patterns\n",
    "    SPE_CHAR_RE = re.compile(\"[^a-zA-ZÀ-ÿ]\")\n",
    "    SPACES_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "    # Convert 'ortho' column to lowercase and strip spaces, then create a dictionary for quick lookup\n",
    "    lexique_dict = {\n",
    "        row[\"ortho\"].lower().strip(): row[\"lemme\"]\n",
    "        for _, row in lexique_filtred_unique.iterrows()\n",
    "    }\n",
    "\n",
    "    # Iterate over each sentence in the text\n",
    "    for sentence in tqdm(text):\n",
    "        if sentence is np.nan:\n",
    "            sentences.append(sentence)\n",
    "            lemmes.append(sentence)\n",
    "        else:\n",
    "            # Replace special characters with spaces while keeping accents\n",
    "            sentence = SPE_CHAR_RE.sub(\" \", sentence)\n",
    "\n",
    "            # Convert sentence to lowercase and split\n",
    "            words = nltk.word_tokenize(sentence.lower(), language=\"french\")\n",
    "\n",
    "            # Look up lemmes in the pre-processed dictionary\n",
    "            lemme_words = [lexique_dict.get(word, word) for word in words]\n",
    "\n",
    "            # Join words back into a sentence and strip leading/trailing spaces\n",
    "            cleaned_sentence = SPACES_RE.sub(\" \", \" \".join(words)).strip()\n",
    "            cleaned_lemmes = SPACES_RE.sub(\" \", \" \".join(lemme_words)).strip()\n",
    "\n",
    "            # Append the cleaned sentence to the list of sentences\n",
    "            sentences.append(cleaned_sentence)\n",
    "            lemmes.append(cleaned_lemmes)\n",
    "\n",
    "    return sentences, lemmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80933b8204b4a09907491ed0887c007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmes</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_lemmes</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>pages de carnet personnalisées olivia dot grid...</td>\n",
       "      <td>page de carnet personnalisé olivia dot grid ca...</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>journal des arts le n du l art et son marche s...</td>\n",
       "      <td>journal des art le ne du l art et son marche s...</td>\n",
       "      <td>177</td>\n",
       "      <td>172</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>731</td>\n",
       "      <td>732</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>peluche donald europe disneyland marionnette à...</td>\n",
       "      <td>peluche donald europe disneyland marionnette à...</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>la guerre des tuques luc a des idées de grande...</td>\n",
       "      <td>la guerre des tuques luc a des idée de grandeu...</td>\n",
       "      <td>203</td>\n",
       "      <td>212</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid                                               text  \\\n",
       "0  3804725264  1263597046  pages de carnet personnalisées olivia dot grid...   \n",
       "1   436067568  1008141237  journal des arts le n du l art et son marche s...   \n",
       "2   201115110   938777978  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3    50418756   457047496  peluche donald europe disneyland marionnette à...   \n",
       "4   278535884  1077757786  la guerre des tuques luc a des idées de grande...   \n",
       "\n",
       "                                              lemmes  len_text  len_lemmes  \\\n",
       "0  page de carnet personnalisé olivia dot grid ca...        68          64   \n",
       "1  journal des art le ne du l art et son marche s...       177         172   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...       731         732   \n",
       "3  peluche donald europe disneyland marionnette à...        52          52   \n",
       "4  la guerre des tuques luc a des idée de grandeu...       203         212   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  \n",
       "2           50  \n",
       "3         1280  \n",
       "4         2705  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_lemmes</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.491600e+04</td>\n",
       "      <td>8.491600e+04</td>\n",
       "      <td>84916.000000</td>\n",
       "      <td>84916.000000</td>\n",
       "      <td>84916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.555468e+09</td>\n",
       "      <td>1.152691e+09</td>\n",
       "      <td>520.531537</td>\n",
       "      <td>517.319422</td>\n",
       "      <td>1773.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.588656e+09</td>\n",
       "      <td>1.751427e+08</td>\n",
       "      <td>664.802838</td>\n",
       "      <td>660.806625</td>\n",
       "      <td>788.179885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.839120e+05</td>\n",
       "      <td>6.728400e+04</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.760519e+08</td>\n",
       "      <td>1.056269e+09</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>1281.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.190506e+09</td>\n",
       "      <td>1.213354e+09</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.995599e+09</td>\n",
       "      <td>1.275646e+09</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>803.000000</td>\n",
       "      <td>2522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.252012e+09</td>\n",
       "      <td>1.328824e+09</td>\n",
       "      <td>11919.000000</td>\n",
       "      <td>12066.000000</td>\n",
       "      <td>2905.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          productid       imageid      len_text    len_lemmes   prdtypecode\n",
       "count  8.491600e+04  8.491600e+04  84916.000000  84916.000000  84916.000000\n",
       "mean   2.555468e+09  1.152691e+09    520.531537    517.319422   1773.219900\n",
       "std    1.588656e+09  1.751427e+08    664.802838    660.806625    788.179885\n",
       "min    1.839120e+05  6.728400e+04      4.000000      4.000000     10.000000\n",
       "25%    6.760519e+08  1.056269e+09     57.000000     56.000000   1281.000000\n",
       "50%    3.190506e+09  1.213354e+09    265.000000    263.000000   1920.000000\n",
       "75%    3.995599e+09  1.275646e+09    806.000000    803.000000   2522.000000\n",
       "max    4.252012e+09  1.328824e+09  11919.000000  12066.000000   2905.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(PATH_PROCESSED + \"X_train_translated.csv\", index_col=0)\n",
    "# X_train[\"text\"] = strip_accents(X_train[\"text\"])\n",
    "X_train[\"text\"], X_train[\"lemmes\"] = clean(X_train[\"text\"])\n",
    "# len texts\n",
    "X_train[\"len_text\"] = X_train[\"text\"].str.len()\n",
    "X_train[\"len_lemmes\"] = X_train[\"lemmes\"].str.len()\n",
    "\n",
    "# add target column\n",
    "X_train[target.columns[0]] = target\n",
    "\n",
    "X_train.drop([\"designation\", \"description\", \"language\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# save dataframe\n",
    "X_train.to_csv(PATH_PROCESSED + \"X_train_preprocessed.csv\")\n",
    "display(X_train.head())\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f44e9e15054adb934f107df880b717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmes</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_lemmes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84916</th>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "      <td>folkmanis puppets marionnette et théâtre mini ...</td>\n",
       "      <td>folkmanis puppets marionnette et théâtre mini ...</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84917</th>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "      <td>porte flamme gaxix flamebringer gaxix u twilig...</td>\n",
       "      <td>porte flamme gaxix flamebringer gaxix u twilig...</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84918</th>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "      <td>pompe de filtration speck badu</td>\n",
       "      <td>pompe de filtration speck badu</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84919</th>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "      <td>robot de piscine électrique ce robot de piscin...</td>\n",
       "      <td>robot de piscine électrique ce robot de piscin...</td>\n",
       "      <td>937</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84920</th>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "      <td>hsm destructeur securio c coupe crois e x mm</td>\n",
       "      <td>hsm destructeur securio c coupe croire 2e x mm</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        productid     imageid  \\\n",
       "84916   516376098  1019294171   \n",
       "84917   133389013  1274228667   \n",
       "84918  4128438366  1295960357   \n",
       "84919  3929899732  1265224052   \n",
       "84920   152993898   940543690   \n",
       "\n",
       "                                                    text  \\\n",
       "84916  folkmanis puppets marionnette et théâtre mini ...   \n",
       "84917  porte flamme gaxix flamebringer gaxix u twilig...   \n",
       "84918                     pompe de filtration speck badu   \n",
       "84919  robot de piscine électrique ce robot de piscin...   \n",
       "84920       hsm destructeur securio c coupe crois e x mm   \n",
       "\n",
       "                                                  lemmes  len_text  len_lemmes  \n",
       "84916  folkmanis puppets marionnette et théâtre mini ...        52          52  \n",
       "84917  porte flamme gaxix flamebringer gaxix u twilig...        63          62  \n",
       "84918                     pompe de filtration speck badu        30          30  \n",
       "84919  robot de piscine électrique ce robot de piscin...       937         934  \n",
       "84920     hsm destructeur securio c coupe croire 2e x mm        44          46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_lemmes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.381200e+04</td>\n",
       "      <td>1.381200e+04</td>\n",
       "      <td>13812.000000</td>\n",
       "      <td>13812.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.549060e+09</td>\n",
       "      <td>1.153300e+09</td>\n",
       "      <td>520.550463</td>\n",
       "      <td>517.218650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.593114e+09</td>\n",
       "      <td>1.704741e+08</td>\n",
       "      <td>695.102422</td>\n",
       "      <td>690.471314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.847940e+05</td>\n",
       "      <td>4.826610e+05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.144059e+08</td>\n",
       "      <td>1.055618e+09</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.195801e+09</td>\n",
       "      <td>1.212607e+09</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.993171e+09</td>\n",
       "      <td>1.275573e+09</td>\n",
       "      <td>805.000000</td>\n",
       "      <td>803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.252011e+09</td>\n",
       "      <td>1.328823e+09</td>\n",
       "      <td>19483.000000</td>\n",
       "      <td>19169.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          productid       imageid      len_text    len_lemmes\n",
       "count  1.381200e+04  1.381200e+04  13812.000000  13812.000000\n",
       "mean   2.549060e+09  1.153300e+09    520.550463    517.218650\n",
       "std    1.593114e+09  1.704741e+08    695.102422    690.471314\n",
       "min    1.847940e+05  4.826610e+05      6.000000      6.000000\n",
       "25%    6.144059e+08  1.055618e+09     56.000000     56.000000\n",
       "50%    3.195801e+09  1.212607e+09    253.000000    250.000000\n",
       "75%    3.993171e+09  1.275573e+09    805.000000    803.000000\n",
       "max    4.252011e+09  1.328823e+09  19483.000000  19169.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(PATH_PROCESSED + \"X_test_translated.csv\", index_col=0)\n",
    "# X_test[\"text\"] = strip_accents(X_test[\"text\"])\n",
    "X_test[\"text\"], X_test[\"lemmes\"] = clean(X_test[\"text\"])\n",
    "# len texts\n",
    "X_test[\"len_text\"] = X_test[\"text\"].str.len()\n",
    "X_test[\"len_lemmes\"] = X_test[\"lemmes\"].str.len()\n",
    "\n",
    "X_test.drop([\"designation\", \"description\", \"language\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# save dataframe\n",
    "X_test.to_csv(PATH_PROCESSED + \"X_test_preprocessed.csv\")\n",
    "display(X_test.head())\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDCLOUD mots normaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in X_train[\"prdtypecode\"].unique():\n",
    "    # make a map of word of X_train['text']\n",
    "    word_map = {}\n",
    "    total_text = \"\"\n",
    "    code_df = X_train[\"text\"].loc[X_train[\"prdtypecode\"] == code]\n",
    "    print(f\"########### {code} ###########\")\n",
    "    for i in tqdm(code_df.index, total=len(code_df)):\n",
    "        text = code_df[i]\n",
    "        if text is not np.nan:\n",
    "            total_text += text + \" \"\n",
    "            for j in text.split(\" \"):\n",
    "                if j not in word_map and j != \"\":\n",
    "                    word_map[j] = 1\n",
    "                elif j != \"\":\n",
    "                    word_map[j] += 1\n",
    "\n",
    "    # sort the map by value\n",
    "    word_map = sorted(word_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Setting up the subplot for bar plot and word cloud\n",
    "    fig, axs = plt.subplots(\n",
    "        1, 2, figsize=FIGSIZE\n",
    "    )  # Corrected to subplots for creating a 1x2 grid\n",
    "    fig.suptitle(f\"Most frequent words of prdtypecode: {code}\")  # Corrected method name\n",
    "\n",
    "    # Bar plot\n",
    "    n_words = 30\n",
    "    axs[0].bar([i[0] for i in word_map[:n_words]], [i[1] for i in word_map[:n_words]])\n",
    "    axs[0].tick_params(\n",
    "        axis=\"x\", rotation=90\n",
    "    )  # Corrected method for setting x-ticks rotation\n",
    "\n",
    "    # Word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=500,\n",
    "        width=640,\n",
    "        height=360,\n",
    "        collocations=False,\n",
    "    ).generate(total_text)\n",
    "    axs[1].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")  # Moved inside the loop to apply to each word cloud subplot\n",
    "\n",
    "    plt.tight_layout(\n",
    "        rect=[0, 0, 1, 0.96]\n",
    "    )  # Adjust layout to not overlap with the suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDCLOUD lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in X_train[\"prdtypecode\"].unique():\n",
    "    # make a map of word of X_train['text']\n",
    "    word_map = {}\n",
    "    total_text = \"\"\n",
    "    code_df = X_train[\"lemmes\"].loc[X_train[\"prdtypecode\"] == code]\n",
    "    print(f\"########### {code} ###########\")\n",
    "    for i in tqdm(code_df.index, total=len(code_df)):\n",
    "        text = code_df[i]\n",
    "        if text is not np.nan:\n",
    "            total_text += text + \" \"\n",
    "            for j in text.split(\" \"):\n",
    "                if j not in word_map and j != \"\":\n",
    "                    word_map[j] = 1\n",
    "                elif j != \"\":\n",
    "                    word_map[j] += 1\n",
    "\n",
    "    # sort the map by value\n",
    "    word_map = sorted(word_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Setting up the subplot for bar plot and word cloud\n",
    "    fig, axs = plt.subplots(\n",
    "        1, 2, figsize=FIGSIZE\n",
    "    )  # Corrected to subplots for creating a 1x2 grid\n",
    "    fig.suptitle(\n",
    "        f\"Most frequent lemmes of prdtypecode: {code}\"\n",
    "    )  # Corrected method name\n",
    "\n",
    "    # Bar plot\n",
    "    n_words = 30\n",
    "    axs[0].bar([i[0] for i in word_map[:n_words]], [i[1] for i in word_map[:n_words]])\n",
    "    axs[0].tick_params(\n",
    "        axis=\"x\", rotation=90\n",
    "    )  # Corrected method for setting x-ticks rotation\n",
    "\n",
    "    # Word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=500,\n",
    "        width=640,\n",
    "        height=360,\n",
    "        collocations=False,\n",
    "    ).generate(total_text)\n",
    "    axs[1].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")  # Moved inside the loop to apply to each word cloud subplot\n",
    "\n",
    "    plt.tight_layout(\n",
    "        rect=[0, 0, 1, 0.96]\n",
    "    )  # Adjust layout to not overlap with the suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDCLOUD sans pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin[target.columns[0]] = target\n",
    "# join texts\n",
    "df_origin[\"text\"] = np.where(\n",
    "    df_origin[\"description\"].isna(),\n",
    "    df_origin[\"designation\"].astype(str),\n",
    "    df_origin[\"designation\"].astype(str) + \" \" + df_origin[\"description\"].astype(str),\n",
    ")\n",
    "for code in df_origin[\"prdtypecode\"].unique():\n",
    "    # make a map of word of X_train['text']\n",
    "    word_map = {}\n",
    "    total_text = \"\"\n",
    "    code_df = df_origin[\"text\"].loc[df_origin[\"prdtypecode\"] == code]\n",
    "    print(f\"########### {code} ###########\")\n",
    "    for i in tqdm(code_df.index, total=len(code_df)):\n",
    "        text = code_df[i]\n",
    "        total_text += text + \" \"\n",
    "        for j in text.split(\" \"):\n",
    "            if j not in word_map and j != \"\":\n",
    "                word_map[j] = 1\n",
    "            elif j != \"\":\n",
    "                word_map[j] += 1\n",
    "\n",
    "    # sort the map by value\n",
    "    word_map = sorted(word_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Setting up the subplot for bar plot and word cloud\n",
    "    fig, axs = plt.subplots(\n",
    "        1, 2, figsize=FIGSIZE\n",
    "    )  # Corrected to subplots for creating a 1x2 grid\n",
    "    fig.suptitle(\n",
    "        f\"Most frequent words (without cleaning) of prdtypecode: {code}\"\n",
    "    )  # Corrected method name\n",
    "\n",
    "    # Bar plot\n",
    "    n_words = 30\n",
    "    axs[0].bar([i[0] for i in word_map[:n_words]], [i[1] for i in word_map[:n_words]])\n",
    "    axs[0].tick_params(\n",
    "        axis=\"x\", rotation=90\n",
    "    )  # Corrected method for setting x-ticks rotation\n",
    "\n",
    "    # Word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=500,\n",
    "        width=640,\n",
    "        height=360,\n",
    "        collocations=False,\n",
    "    ).generate(total_text)\n",
    "    axs[1].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")  # Moved inside the loop to apply to each word cloud subplot\n",
    "\n",
    "    plt.tight_layout(\n",
    "        rect=[0, 0, 1, 0.96]\n",
    "    )  # Adjust layout to not overlap with the suptitle\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
